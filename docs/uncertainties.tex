\documentclass{scrartcl}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\renewcommand{\lstlistingname}{Listing}
\title{Fitting Parameter Uncertainties}
\subtitle{Version 1.0}
\author{R. Steven Turley}
\date{September 1, 2018}
\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
This article derives the formulas for estimating parameter
uncertainties in least square fitting to data. It relies heavily
on Bevington\cite{Bevington}, the first edition of which
served as my introduction to the subject as an undergraduate.
There is additional helpful background on Wikipedia\cite{Wikipedia}
and MathWorld\cite{MathWorld}. I will use that same notation
as in my article on linear least squares fitting\cite{linLS}
which is an inter-related companion to this one.

The goal in least squares is to find the best fit to a function
of the form $f(x;\vec{b})$
to a set of data points $(x_i,y_i)$.
It is called "least squares" because by "best fit" I
mean the function which finds the set
of $p$ parameters $b_k$ which minimizes $\chi^2$, the sum of the squares of
the differences between $f(x_i;\vec{b})$ and $y_i$.
\begin{equation}
\chi^2 = \sum_{i=1}^n [f(x_i;\vec{b})-y_i)]^2\label{eq:uwchi}
\end{equation}
If the data is heteroscedactic (i.e. if the extent of the deviations
of $y_i$ from $f(x_i,\vec{b})$ varies across the range of $x_i$, 
the appropriate function to minimize is a weighted sum of the squares.
This can be written in terms of the weights $w_i$ or in terms of the
relative uncertainties at each data point $\sigma_i$.
\begin{equation}
\chi^2 \ \sum_i [w_i (x_i;\vec{b})-y_i)]^2\label{eq:wchi}
\
\end{equation}
Note that it defined $w_i$ differently here than in the linear
least squares paper\cite{linLS} to match a convention in the
Julia library code \texttt{LsqFit.jl} which I modified to use
for these studies.

\section{Estimating Uncertainties}
The estimated uncertainty in the measured data $s$ can be calculated
from $\chi^2$.
\begin{align}
s &= \sqrt{\frac{\chi^2}{n-p}}\\
&= \sqrt{\frac{1}{n-p}\sum_{i=1}^n [f(x_i;\vec{b})-y_i)]^2}\label{eq:sx}
\end{align}
If we expand $f(y;\vec{b})$ in a Taylor series about
the mean value $\bar{y}$,
\begin{align}
y_i &= f(x_i;\vec{b})\\
 &\approx \bar{y}+\sum_k(b_k-\bar{b}_k)\left(
	\frac{\partial f}{\partial b_k}\right) +\cdots\\
y_i - \bar{y} &= \sum_k(b_k-\bar{b}_k)\left(
	\frac{\partial f}{\partial b_k}\right) +\cdots \label{eq:ydif}
\end{align}
Substituting Eq.~\ref{eq:ydif} into Eq.~\ref{eq:sx} and keeping
only the first order terms,
\begin{align}
s_y^2 &= \frac{1}{n-p}\sum_{i=1}^n(y_i-\bar{y})^2\\
	&\approx \frac{1}{n-p}\left[\sum_{i,k}(b_k-\bar{b}_k)\left(
		\frac{\partial f}{\partial f}{\partial b_k}\right)\right]^2.
		\label{eq:sy2}
\end{align}
If we define $C_{ij}$ to be the elements of the
covariance matrix with
\begin{equation}
C_{ij} \equiv \frac{1}{n-p}\sum_i(b_i-\bar{b}_i)^2,
\end{equation}
we note the the uncertainty in the fit parameter $b_i$ is $s_i$ and
\begin{equation}
s_i^2 = C_{ii}.
\end{equation}
Thus the diagonal elements of the covariance matrix are the uncertainties
we are seeking. The off-diagonal elements show the statistical
correlations between the fit parameters.
The Jacobian $J$ of $f$ is defined to be
\begin{equation}
J = \left(\begin{array}{ccc}
\partial f(x_1;\vec{b})/\partial b_1& \cdots &
	\partial f(x_1;\vec{b})/\partial b_p\\
\vdots & \vdots & \vdots \\
\partial f(x_n; \vec{b})/\partial b_1 & \cdots  &
	\partial f(x_n;\vec{b})/\partial b_p
\end{array}\right).
\end{equation}
If we expand the quadratic in Eq.~\ref{eq:sy2} and write the matrix
$C$ as
\begin{equation}
C = \left(\begin{array}{ccccc}
c_{11} & c_{12} & \cdots & c_{1,p-1} & c_{1,p} \\
c_{21} & c_{22} & \cdots & c_{2,p-1} & c_{2,p} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
c_{p-1,1} & c_{p-1,2} & \cdots & c_{p-1,p-1} & c_{p-1,p} \\
c_{p1} & c_{p2} & \cdots & c_{p,p-1} & c_{p,p}
\end{array}\right)
\end{equation}
and represent the identity matrix as $I$, then we can write
Eq.~\ref{eq:sy2} as
\begin{align}
s_y I &= C (J^{T}J)\\
C &= s_y (J^{T}J)^{-1}\\
\sigma_i &\approx \sqrt{C_{ii}}.
\end{align}
Note that if the $f$ is linear in the parameters $b_k$, then
$f$ does not have any derivatives of higher order than 1. In
this case, Eq.~\ref{eq:ydif} is exact. However the computed
uncertainties in the fit parameters are still an estimate since
$s_y$ only equals $\sigma_y$
and $s_i$ only equals $\sigma_i$
in the limit as $n\rightarrow\infty$.

\section{Numerical Examples}
I will consider two numerical examples which demonstrate how
well this formula works.
\subsection{Linear Fit}
Consider the function
\begin{equation}
f(x; b_1. b_2) = b_1 + b_2 x.
\end{equation}
In other words,
\begin{align}
g_1(x) &= 1\\
g_2(x) &= x.
\end{align}
The Jacobian $J$ has the elements
\begin{align}
J_{i,1} &= 1\\
J_{i,2} &= x_i\\
J &= \left(\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_{n-2} \\
1 & x_{n-1}
\end{array}\right)\\
J^{T}J &= \left(\begin{array}{cc}
\sum x_i & \sum x_i^2 \\
N & \
\sum x_i \end{array}\right)
\end{align}
Using the result from my linear least squares article\cite{linLS}
or polynomial fitting article\cite{polyfit}, the solution for the fit
parameters is the solution to
\begin{equation}
Ab=y
\end{equation}
where
\begin{align}
A &= \left(\begin{array}{cc}
N & \sum x_i \\
\sum x_i & \sum x_i^2 \end{array}\right)\\
y &= \left(\begin{array}{c}
\sum y_i \\
\sum y_i x_i
\end{array}\right).
\end{align}
To get good approximations, I'll use an array with 1,000
points from 0 to 1 for $x$ and $m=1.5$, $b=3.0$. The random
noise will have an amplitude of 0.2. Here is the Julia code
for the implementation.

The function \texttt{lfit} fits 1,000 points to the line
described in the previous paragraph. It returns the fit parameters
in column 1 of the returned matrix and the estimated uncertainties
of the parameters in column 2.
\begin{lstlisting}
using LinearAlgebra
using Printf

function lfit()
    npts = 10000
    xpts = [(i-1.0)/(npts-1) for i=1:npts]
    m = 1.5
    b = 3.0
    ypts = b .+ m.*xpts .+ randn(npts)*0.2
    A = Matrix{Union{Missing, Float64}}(missing, 2, 2)
    y = Array{Union{Missing, Float64}}(missing, 2)
    A[1,1] = npts
    A[1,2] = sum(xpts)
    A[2,1] = A[1,2]
    A[2,2] = sum(xpts.*xpts)
    y[1] = sum(ypts)
    y[2] = sum(ypts.*xpts)
    b = A\y
    J = hcat(ones(npts),xpts)
    yfit = b[1] .+ xpts.*b[2]
    res = ypts .- yfit
    sigmay = sqrt.(sum(abs2,res)/(npts-2))
    cov = inv(J'*J)
    sigmai = sigmay.*sqrt.(diag(cov))
    [b sigmai]
end
\end{lstlisting}
The following code calls \texttt{lfit} 1,000 times
to compute the fit parameters with successive sets
of random noise. With n measurements of $x$
\begin{equation}
\sigma_x = \sqrt{\langle x^2\rangle - \langle x\rangle^2}.
\end{equation}
The sums for the means of the fit parameters are accumulated
in \texttt{psum}. The sums for the means of the squares of
the fit parameters are accumulated in \texttt{sumsq}.
\begin{lstlisting}
# repeat 1000 times for averages
let
    psum = zeros(2,2)
    sumsq = zeros(2,2)
    trials = 1000
    for i=1:trials
        ft = lfit()
        psum += ft
        sumsq += ft.*ft
    end
    pbar = psum./trials
    pbarsq = sumsq./trials
    sigma = sqrt.(pbarsq[:,1] .- pbar[:,1].^2)
    @printf("average intercept = %.3f +/- %.4f\n",
        pbar[1,1], pbar[1,2])
    @printf("average slope = %.3f +/- %.4f\n",
        pbar[2,1], pbar[2,2])
    @printf("computed uncertainty in intercept: %.2e\n",
        sigma[1])
    @printf("computed uncertainty in slope: %.2e\n",
        sigma[2])
end
\end{lstlisting}
This code produced the following output:
\begin{lstlisting}
average intercept = 3.000 +/- 0.0040
average slope = 1.500 +/- 0.0069
computed uncertainty in intercept: 3.94e-03
computed uncertainty in slope: 6.89e-03
\end{lstlisting}
As you can see, the average of the fit parameters agreed
very well with the model. The variance in the fit
parameters agreed well with the estimated variances using
the formulas in this article.

\subsection{Nonlinear Fit}
The second fit I tried was a fit to the nonlinear function
\begin{equation}
f(x)= \frac{b_1}{b_2 + cos(2\pi x/b_3)}
\end{equation}
with $b_1 = 0.5$, $b_2 = 1.35$, and $b_3 = 0.3$. I won't
go into the details of the fitting since that's implemented in the
LsqFit module which is documented in the linear least squares
article\cite{linLS}. The routine curve\_fit returns a structure
with the degrees of freedom, the Jacobian, and the mean square
error which I will use to estimate the fit error.

Here is the \texttt{nlfit} function which did a single nonlinear
fit. Because the function was more difficult to fit and nonlinear
fitting is an iterative process, this function took a lot
longer to run than in the linear case.
\begin{lstlisting}
using LsqFit
using Printf
using LinearAlgebra

function nlfit()
    npts = 10000
    f(x,p) = p[1]./(p[2] .+ cos.(2*pi .* x ./p[3]))
    xpts = [(i-1.0)/(npts-1) for i=1:npts]
    p0 = [0.5,1.35,0.3]
    ypts = f(xpts,p0)+randn(npts)/0.2
    wt = ones(npts)
    cf = curve_fit(f, xpts, ypts, wt, p0)
    sigmay = sqrt.(sum(abs2,cf.resid)/(cf.dof))
    J = cf.jacobian
    cov = inv(J'*J)
    sigmai = sigmay.*sqrt.(diag(cov))
    [cf.param sigmai]
end
\end{lstlisting}
This function was called in much the same way that \texttt{lfit}
was called for linear fits. The major difference is that we are
fitting three parameters this time instead of two.
\begin{lstlisting}
let
    psum = zeros(3,2)
    sumsq = zeros(3,2)
    trials = 1000
    for i=1:trials
        ft = nlfit()
        psum += ft
        sumsq += ft.*ft
    end
    pbar = psum./trials
    pbarsq = sumsq./trials
    sigma = sqrt.(pbarsq[:,1] .- pbar[:,1].^2)
    @printf("average b1 = %.3f +/- %.4f\n",
        pbar[1,1], pbar[1,2])
    @printf("average b2 = %.3f +/- %.4f\n",
        pbar[2,1], pbar[2,2])
    @printf("average b3 = %.3f +/- %.4f\n",
            pbar[3,1], pbar[3,2])
    @printf("computed uncertainty in b1: %.2e\n",
        sigma[1])
    @printf("computed uncertainty in b2: %.2e\n",
        sigma[2])
        @printf("computed uncertainty in b3: %.2e\n",
            sigma[3])
end
\end{lstlisting}
The computed uncertainties were not as close to the
average uncertainties in this case as they were with a linear
fit using fewer parameters, but they are reasonable.
\begin{lstlisting}
average b1 = 0.508 +/- 0.1154
average b2 = 1.362 +/- 0.1080
average b3 = 0.297 +/- 0.0027
computed uncertainty in b1: 1.24e-01
computed uncertainty in b2: 1.19e-01
computed uncertainty in b3: 4.20e-02
\end{lstlisting}

\begin{thebibliography}{9}

\bibitem{Bevington}
Philip R.~Bevington, D.~Keith Robinson, 
"Data Reduction and Error Analysis for the Physical Sciences,"
Third Edition, McGraw Hill, 2003.

\bibitem{Wikipedia}
Wikipedia, "Monotonic least squares,"
\url{https://en.wikipedia.org/wiki/Linear_least_squares}
(accessed 31 Aug 2018).

\bibitem{MathWorld}
Eric W.~ Weisstein, "Least Squares Fitting," From MathWorld--A Wolfram
Web Resource.
\url{http://mathworld.wolfram.com/LeastSquaresFitting.html}
(accessed 31 Aug 2018).

\bibitem{linLS}
R.~Steven Turley, "Linear Least Squares," BYU, 2018.

\bibitem{polyfit}
R.~Steven Turley, "Polynomial Fitting," BYU, 2018.

\end{thebibliography}

\end{document}
